{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Restart - Balsa Testing - 2020-06-14.ipynb",
   "provenance": [],
   "collapsed_sections": [
    "H1uq0RMITXy3",
    "MDnksm1MTmV7",
    "38KeztwBajb8",
    "gXxIvzREazRq",
    "7AulXwNtb5yd",
    "GDR12BB4pPd1"
   ],
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "",
   "display_name": ""
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/restartus/covid-projection/blob/rich-v1.5/nb/Restart_Balsa_Testing_2020_06_14.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UPP19bJbIKev",
    "colab_type": "text"
   },
   "source": [
    "---\n",
    "\n",
    "_Proprietary and Confidential. Do not distribute without permission._\n",
    "\n",
    "---\n",
    "\n",
    "# Restart Partners  \n",
    "\n",
    "---\n",
    "_To:_ Jun Amora (Mayors Office, City of New York)  \n",
    "_From:_ Bharat Shyam, Rich Tong (Restart)  \n",
    "_Re:_ Analysis for NYC PPE needs  \n",
    "_Date:_ 20 May 2020  \n",
    "\n",
    "--- \n",
    "\n",
    "New York City needs a 90-day stockpile for the heathcare workers, first responders and congregate care facilities is really important, but coming up with an estimate for this is difficult given the variability of the infection and the uncertainty in the degree of economic recovery and social mobility. Therefore, we are providing another resource model to augment yours that shows that our figures are within 30%-50% of your bottoms estimate. Given that we are happy to:\n",
    "\n",
    "- _Refine healthcare estimates_. All models are heavily dependent on estimates of population involved and usage data. \n",
    "- _Non-healthcare estimates_. For instance, this model does project needs outside of the healthcare area such as small business, vertical industries and vulnerable populations.\n",
    "- _Long-term modeling_. We are extending the model to include test equipment, disinfectant wipes and liquid disinfectants, so happy to add things that you need. Also we will be integrating epi and economic models too and would love to partner with you on that.\n",
    "\n",
    "Given the uncertainties involved, this might help you make the right estimates. What follows next are:\n",
    "\n",
    "1. Disclaimer. This is not a definitive estimate. You should use other sources and information to make your decisions.\n",
    "2. Data Sources. We have included the model source data, how the model is constructed and then results. Feel free to use this data and modify as appropriate, but it serves as documentation for all the assumptions made.\n",
    "3. Model. The way the calculation is done with assumptions and resulting projections\n",
    "4. Outputs. The conclusions we can draw from the projection.\n",
    "\n",
    "## Disclaimer\n",
    "It must be noted that the Restart Partners (\"Restart\") Equipment Model (the \"Model\") is made available for public use free of charge. Determining equipment needs for each jurisdiction, entity or other party (each a \"User\") is a complex and multifaceted decision process. Restart does not does not have the authority or ability to assign empirical risks levels nor make definitive use decisions for any User. Rather, the Model provides one approach to making recommendations that can help Users make decisions about their potential equipment uses by allowing them to calculate their potential requirements. Users are strongly encouraged to consider other sources of information and expressly disclaim any cause of action they may have against Restart arising from or relating to the Model or its analysis. Implementing the equipment levels projected by the Model will not eliminate the risk of COID-19 cases being linked to activites in an economy or workplace. In this context, it is important to note that this equipment alone will not eliminate the risk of infection. All Users should remain informed about and abide by any decisions made by local public health and government authorities regarding specific mitigation efforts, including equipment in the model, as the situation is dynamic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Xo1YI-GvU4o",
    "colab_type": "text"
   },
   "source": [
    "# Model Data\n",
    "\n",
    "Because we do not have New York City specific data, we used various open data sources to fill in the five major assumptions in the model:\n",
    "\n",
    "1. Usage by Population. This cuts the item usage per person per day. This right now is a series of levels. So we have four levels for civilians and then two levels for healthcare workers.\n",
    "2. Usage per Patient. This is the way Epidemiological models work. That is, given a number of patients, calculate how much they will need. The model currently uses the [WHO Surge Essential Supplies Forecasting Tool v1.2](https://www.who.int/emergencies/diseases/novel-coronavirus-2019/technical-guidance/covid-19-critical-items) and estimates the entire US population use with 1,000 cases and fast transmission and slow response. So this is a very pessimistic scenario. This makes sense when calculating the surge estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DOPqqurOE8-E",
    "colab_type": "text"
   },
   "source": [
    "# Strategy for the Model\n",
    "\n",
    "The next steps are a little complicated, but we are converting the entire model into a series of vectorized operations. \n",
    "\n",
    "## Daily Usage of Equipment by Protection levels D[l, n]\n",
    "\n",
    "So first we need the inputs which are the list of protection levels p x the number of items we are tracking n. So this is an p x n matrix where each entry says for protection level l, we have so many items per capita per day. So the first data list is Daily_usage D = l rows x n columns or D.shape = ( l, n )\n",
    "\n",
    "In Python speak this is Usage_pd since it is a _P_anda _D_ataframe. Right now this is a test matrix that is constructed in the Jupyter Notebook, but longer term, it should be pulled from an database with the suitable annotations.\n",
    "\n",
    "## Sub-Population Count vector P[p, 1]\n",
    "\n",
    "In the original model, we had p sub-populations. In the simplest model, it was just two populations: non-employees of healthcare companies and employees. With SOC and other codes, there are close to a thousands. So the populations are a vector that includes a description and a population count.\n",
    "\n",
    "So for example P = [ 7400, 435 ] which is just about the right numbers for Seattle and is a [ 1, p ] vector\n",
    "\n",
    "## Sub-Population Usage of Protection Levels U[p, l]\n",
    "\n",
    "You can think of this as a one-hot matrix in the most simple form. That is for each subpopulation, what level of protection do you need. \n",
    "\n",
    "For example, in the simplest case, if there are six protection levels, then if non-employees get level 1 and healthcare employees get level 6, then the matrix looks like:\n",
    "\n",
    "     0 1 0 0 0 0 0\n",
    "     0 0 0 0 0 0 1\n",
    "\n",
    "While date entry is complicated, this let's you take any given population and give it fractions of protection. For instance, if a healthcare employer typically had 50% of it's workers as office workers at level 2, 25% as customer facing and 10% taking care of non COVID and 15% in direct contact, that vector would look like:\n",
    "\n",
    "    0 0.5 0.25 0 0 0.10 0.15\n",
    "\n",
    "This gives the modeler great flexibility with employers or any population\n",
    "\n",
    "The matrix of usage U looks like p rows and l columns\n",
    "\n",
    "    Usage_pd.shape = [ p, l ]\n",
    "\n",
    "## Required Equipment per capita per sub-population R[p, n]\n",
    "\n",
    "So with this, you can see that with a single operation you can get to the actual equipment levels require per person per day for a given population. Note that there is new Python 3.5 syntax for [matrix multiply](https://docs.python.org/3/whatsnew/3.5.html#whatsnew-pep-465)\n",
    "\n",
    "    U x D = [ p, l ] x [l, n] = R[p, n]\n",
    "    # in the new Python 3.5 syntax using ampersand\n",
    "    # np.dot for matrices but not tensors\n",
    "    R = U @ D\n",
    "    R = U.dot(D)\n",
    "\n",
    "In Python Numpy speak, we are doing a matrix [multiply](https://www.tutorialexample.com/understand-numpy-np-multiply-np-dot-and-operation-a-beginner-guide-numpy-tutorial/)\n",
    "\n",
    "## Total required equipment for a sub-population T[p, n]\n",
    "\n",
    "Now that we have the per-capita requirements, we need to do a scalar multiply by row\n",
    "\n",
    "    R[p, n] x P[p, 1] = T[p, n]\n",
    "    # Or in python using broadcasting which extends P out n columns\n",
    "    T = R * P\n",
    "\n",
    "## Merging populations into fewer buckets with Essential index by population E[e, p]\n",
    "\n",
    "Many times the subpopulations are going to be too large to understand. For instance when there are 800 job classfied by SOC or where there are 350 employer class by NAICS-6, so for convenience, we define essential levels. You can think of the of this as for each population, where do they fit in where they start. Essential (which has changed since version 1.x) can be thought of as the time period of start. So Essential 0 (like Defcon 1), is the most important and so forth. \n",
    "\n",
    "This let's you stage start up, so for the example subpopulations of non-healthcare employed and heathcare employed, it might look like a simple matrix across 6 start periods as or more analytically E is e rows and p columns.\n",
    "\n",
    "So in the example, it says the first population starts at time 0 and then the second starts in week 6\n",
    "\n",
    "    1 0\n",
    "    0 0\n",
    "    0 0\n",
    "    0 0\n",
    "    0 0\n",
    "    0 0\n",
    "    0 1\n",
    "\n",
    "But this system also allows a stageed restart, so for example, if you want have the workers to come back in the next period for healthcare employees and this series could even be generated as a lambda with any arbitrary function, so in this example, population 1 starts 50% in week 0, then slows continues. \n",
    "\n",
    "While population 2 doesn't start until week 4 and tails up\n",
    "\n",
    "    0.5 0\n",
    "    0.4 0\n",
    "    0.1 0\n",
    "    0 0\n",
    "    0 0.2\n",
    "    0 0.4\n",
    "    0 0.6\n",
    "\n",
    "## Requirements by essential index Re[e, n]\n",
    "\n",
    "In some sense we are doing compression by this, so we are looking at Essential index e is much less than the number of populations p. Or more succinctly e << p and we can get to E with a transpose\n",
    "\n",
    "    E[e, p] x T[p, n] = e x p * p x n = R[e, n]\n",
    "    # In python this looks like\n",
    "    Re = E @ T\n",
    "\n",
    "## Then there is a Cost per item per essential row\n",
    "\n",
    "For simplicity we can assume that each essential level has different costs. In reality, the costs will actually be more complicate and C[p, n] which is much more complicate.d\n",
    "\n",
    "For each item, what is the cost for N95 is $3 and non-surgical is $0.50\n",
    "    C[e, n] = [ $3, $0.50 ]\n",
    "\n",
    "## The easy parts Required Cost per day RC[e, n] and Stockpile S[e, n]\n",
    "\n",
    "OK that was the hard stuff, with these matrices reduced to essential levels and the equipment needed for each, there are just some simple scalar multiplies to get where Cost is a row vector that is all the costsj which is a element-wide multiplication.\n",
    "\n",
    "## Then the Stockpile need by essential levels is SE[e,1]\n",
    "\n",
    "So for each essential you need a different stockpile. Usually more essential needs more levels\n",
    "\n",
    "## Gross cost for equipment by level and stockpile by essential\n",
    "\n",
    "So both the cross cost and the stockpile are done by level as a element-wide multiplication.\n",
    "\n",
    "    Gross cost for the equipment = RC[e, n] = R[e, n] * C[e, n].value\n",
    "    Stockpile needed for d Days = S[e, n] = R[e, n] * SE[e, 1].value\n",
    "\n",
    "Obviously you may not want to stock pile for all e Essential levels, so you just select what you want for instance S[0] will give you the stockpile needs for the most essential level 0.\n",
    "\n",
    "## Handling disinfection, goggle types and reuse\n",
    "\n",
    "We can handle any arbitrary vector of items, just change the list. Longer term, we will have our own GUID system, but right no rely on unique text strings\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j_So41jQQXVE",
    "colab_type": "text"
   },
   "source": [
    "# Moving Beyond Surge in v2\n",
    "\n",
    "There are four major goals for V2.x:\n",
    "\n",
    "Time series forecast based of what's needed so that all matrices now have a time dimension so you can see things change with time.\n",
    "\n",
    "Explicit utilization models beyond per capita the two major ones are\n",
    "    - per \"run or 911 call\" for things that are incident based. This means that every population is going to have an activity matrix in addition to the population matrix\n",
    "    - per COVID PUD (Patient Under investigation) mainly for testing tempos so there will be a matrix of how many patients there are in a given population\n",
    "\n",
    "The time series particularly for activity is based on social mobility model that is handled off this model, but comes in as a matrix SM[p, t] which is how each social mobility population changes for time indexed on 100% at Feb 7.\n",
    "\n",
    "Time series that is the disease progression from SIR or other Epi model which modeled as DP[p, t] so you can look at various populations with differential infection rates.\n",
    "\n",
    "## The first step extending with an time with Social mobility\n",
    "\n",
    "This is first step, the basic strategy is to take the Population needs E[p, n] and add a third dimension E[p, n, t] where t is the time (arbitrarily we are saying weeks for now, but could be days).\n",
    "\n",
    "So the basic input into this most simplistic model, we assume we get a vector that shows social mobility as a function of time for each population $SM[p, t]$ where each row is a population percentage of recovery, that is if say Feb 7 2020 was 100% activity in a given population, then we see if the first population doesn't start until week 3 and then ramps, mobility would look like if say non-healthcare was row 0 and healthcare workers are row 1\n",
    "\n",
    "    [ [ 0.0, 0.0, 0.3, 0.4 ],\n",
    "      [ 0.4, 0.5, 0.6, 0.7 ]]\n",
    "\n",
    "Generating this social mobility projection is easy backwards looking with things like the IHME social index, the trick is a simple projection forward. \n",
    "\n",
    "In this model, we will by default use a simple [sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function) to model this as a placeholder and start them at different points so the Social Mobility Model Input parameters (thank goodness for [Latex Markdown](https://en.wikibooks.org/wiki/LaTeX/Mathematics), [Medium](https://towardsdatascience.com/latex-for-data-scientists-in-under-6-minutes-3815d973c05c), [Latex4Technics](https://www.latex4technics.com/?note=gw021j)\n",
    "\n",
    "While there are many types of [Sigmoids](https://www.quora.com/Is-there-any-difference-between-sigmoid-logistic-and-tanh-function) like tanh, we will use the logistic since it is commonly used in machine learning. The logistic function originally came from population research and it is theoretically [optimal](https://www.quora.com/q/kvuotomuzenzevuw/Logistic-Softmax-Regression) if we ever apply optimization to the problem\n",
    "\n",
    "So for every population we end with a simple starter S[p, 2] which carries the three parameters a or the maximum height, and b the start offset. This is convenient encapsulated in [Scipy Logistic](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.logistic.html)\n",
    "\n",
    "$S(a, b, t) = a \\frac{1}{1 + e^{-t}} + b$\n",
    "\n",
    "From this its easy to generate the actual SM[p, t] = S(a, b, t)\n",
    "\n",
    "Or in real python where a is scale and bj is loc for location\n",
    "\n",
    "    from scipy.stats import logistic\n",
    "    logistic.cdf(t, loc=0, scale=2)\n",
    "\n",
    "With this we can then generate the Essentials Items as E[p, n, t] which is then just that for each slice E[p, n, 0] = E[p, n] * SM[p, 0].values with casting\n",
    "\n",
    "This can be vectorized as well E[p, n] * S[p, t] = ET [p, n, t]as an example with the simplest example of two populations where the non-healthworkers use 500 non-ASTM masks and the healthcare workers use 100 at full surge, \n",
    "\n",
    "    [[ 0, 500],\n",
    "     [ 100, 0 ]]\n",
    "\n",
    "Then if the start sequence looks like the non-healthworker are idle in the first week and then go to 50% in the second, while healthcare workers start at 30% and go to 70%\n",
    "\n",
    "    [ [ 0, 0.5 ],\n",
    "      [ 0.3, 0.7]]\n",
    "\n",
    "The math looks like doing this with a broadcast, so you extend E first with a stack ET[p, n, t ] = E[p, n].stack(t) so you get identical\n",
    "\n",
    "Then you can do element wise math\n",
    "\n",
    "## The next step carrying different run rates\n",
    "\n",
    "First we are explicitly going to change the modeling to not just carry essential items E, but actually a bunch of factors including that are for each population, so the E[p, n] becomes a super set of population characteristics m along with daily use so A\n",
    "\n",
    "$A[p, n, m] = E[p, n] || C[p, m]$\n",
    "\n",
    "So what is in the characteristic C[p, m]\n",
    "  * Number of COVID-19 Patients in a give population for healthcare usage based on patients\n",
    "  * Number of Patients Under Investigation\n",
    "  * Number of Activities by the population which is a different tempo\n",
    "  * Number of \n",
    "\n",
    "This allows a different use\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_o_lYYWBgOsH",
    "colab_type": "text"
   },
   "source": [
    "# Detail of Model\n",
    "\n",
    "These are the details of the model. It is a good example of the parameters that you will need to add. Make sure that you have good advice from medical authorities when looking over these parameters"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TU4PflJ5b2zK",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Get libraries\n",
    "import pandas as pd\n",
    "import numpy as np"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FPZYjkTlg8CS",
    "colab_type": "code",
    "cellView": "form",
    "colab": {}
   },
   "source": [
    "# https://colab.research.google.com/notebooks/forms.ipynb#scrollTo=ZCEBZPwUDGOg\n",
    "#@title Basic Model Parameters\n",
    "#@markdown ####Enter Model Description here:\n",
    "\n",
    "model_name = 'NYC Surge Forecast'  #@param\n",
    "model_description = 'v1.4 WHO Surge'  #@param {type: \"string\"}\n",
    "recurrence_index = 45  #@param {type: \"slider\", min: 0, max: 100}\n",
    "recovery_index = 62  #@param {type: \"slider\", min: 0, max: 100}\n",
    "revision =   103#@param {type: \"number\"}\n",
    "date = '2020-05-20'  #@param {type: \"date\"}\n",
    "model_type = \"surge\"  #@param ['surge', '3-month', '6-month']\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown ####Daily Usage of Equipment Per Person\n",
    "units = \"10,000\" #@param [\"1,000,000\", \"100,000\", \"10,000\", \"1,000\"] {allow-input: true}\n",
    "\n",
    "#@markdown ---"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yLFSWaOVi7tR",
    "colab_type": "text"
   },
   "source": [
    "## Daily Usage of Equipment __n__ by Protection levels __l__ is D[l, n]"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1xwe8g08yRbG",
    "colab_type": "code",
    "outputId": "0cfe4400-e761-40cc-945d-12f70b1717d5",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    }
   },
   "source": [
    "# Eventually we will do this from a database import, but for now, let's use\n",
    "# the data that is normally in the Excel sheet and just recreate \n",
    "# https://colab.research.google.com/drive/1Bcx54NQePYt88RWWmODrRA1pxz-2tnNW?authuser=5#scrollTo=1xwe8g08yRbG\n",
    "\n",
    "# Using PEP https://www.python.org/dev/peps/pep-0008/\n",
    "# For simplicity do as a dictionary\n",
    "Item_name = [\n",
    "              'N95 Surgical Respirator',\n",
    "              'N95 Mask',\n",
    "              'ASTM 3 Surgical Mask',\n",
    "              'ASTM 1-2 Surgical Mask',\n",
    "              'Non-ASTM Mask'\n",
    "              'Reusable Cotton Mask'\n",
    "              'Cotton Mask with Ear Loop',\n",
    "              'Face Shield',\n",
    "              'Goggles'\n",
    "              'Gown',\n",
    "              'Gloves',\n",
    "              'Shoe Covers',\n",
    "              'Test Kits',\n",
    "              'Disinfectant (30ml)',\n",
    "              'Disinfectant wipes'\n",
    "            ]\n",
    "\n",
    "# For this demo, we will just test with two\n",
    "Level_name = [ 'WA0', 'WA1', 'WA2', 'WA3', 'WA4', 'WA5', 'WA6']\n",
    "Item_name = [ 'N95 Surgical', 'non ASTM Mask']\n",
    "print('Item_names', Item_name)\n",
    "Daily_usage_matrix = [\n",
    "                [ 0, 0 ],\n",
    "                [ 0, 1 ],\n",
    "                [ 0, 2 ],\n",
    "                [ 0.1, 3],\n",
    "                [ 0.2, 4],\n",
    "                [ 0.3, 6],\n",
    "                [ 1.18, 0]]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fFHD-0I8JU8N",
    "colab_type": "text"
   },
   "source": [
    "### Daily Usage Matrix verification and conversion to Dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2uM7R8IXJTQE",
    "colab_type": "code",
    "outputId": "3284a762-6ca9-40b7-94b6-526bdd686e73",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 467
    }
   },
   "source": [
    "print('Daily_usage_matrix', Daily_usage_matrix)\n",
    "\n",
    "Daily_usage_df = pd.DataFrame(Daily_usage_matrix,\n",
    "                              columns = Item_name,\n",
    "                              index = Level_name)\n",
    "\n",
    "# use these counts to check the matrix vector bugs\n",
    "level_count = Daily_usage_df.shape[0]\n",
    "item_count = Daily_usage_df.shape[1]\n",
    "print('usage_pd shape is ', Daily_usage_df.shape,\n",
    "      'protection level count is ', level_count,\n",
    "      'item count is ', item_count)\n",
    "\n",
    "print('Daily_usage_pd', Daily_usage_df)\n",
    "\n",
    "Daily_N95s_usage = Daily_usage_df['N95 Surgical']\n",
    "print('Daily N95 Surgical Usage', Daily_N95s_usage)\n",
    "\n",
    "# https://stackoverflow.com/questions/13187778/convert-pandas-dataframe-to-numpy-array\n",
    "print('Daily usage value in Dataframe', Daily_usage_df.values)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p4uMRbROcqpf",
    "colab_type": "text"
   },
   "source": [
    "## Population Data by sub-populations p is P[p, 1]\n",
    "\n",
    "Start with the simplest assumption, two populations, one that is `WA6` and one that is `WA2` as an example. But we will insert more data later once we decide the data source."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "odyDmPbkc3l2",
    "colab_type": "code",
    "outputId": "2ad71ea4-bbda-4ee1-b55c-f7a8a962044c",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    }
   },
   "source": [
    "# This is a dummy test case, later we will use extraction first form a\n",
    "# spreadsheet and then eventually from a data store that is reliable\n",
    "# And which has revision control\n",
    "\n",
    "Population_name = ['Healthcare employees', 'Non employees of healthcare companies']\n",
    "Population_data = [735.2, 7179.6]\n",
    "\n",
    "print('Population Data', Population_data)\n",
    "\n",
    "Population_df = pd.DataFrame(Population_data, index = Population_name, columns = ['Population'])\n",
    "population_count = Population_df.shape[0]\n",
    "print('population count p', population_count)\n",
    "print(Population_df)\n",
    "\n",
    "# https://note.nkmk.me/en/python-type-isinstance/\n",
    "print('type of Population_name', type(Population_name))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hqG2MmDelJZs",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    ""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mxOgyfxTet7N",
    "colab_type": "text"
   },
   "source": [
    "# Usage of PPE by Sub-population p is U[p, l]\n",
    "\n",
    "Now we have a vector which are the population usages and we have a list of needs, so we need to do a matrix multiply of population by needs. Each entry is the percentage of a population at a given level.\n",
    "\n",
    "So in this example, 50% of healthcare workers are level 5 and 50% are at level 6"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "NA65fr95e8w0",
    "colab_type": "code",
    "outputId": "d5e01f62-c49d-44ec-c6e0-33a44877a5d2",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    }
   },
   "source": [
    "# Now we need a matrix which is the pop_type x usage_type and the coefficient is just how much is needed for each\n",
    "# Do this for simplicity start with a zero matrix, we will actually load the data\n",
    "\n",
    "Usage_by_population_matrix = np.zeros([population_count, level_count])\n",
    "\n",
    "Usage_by_population_matrix[1,1] = 1.0\n",
    "Usage_by_population_matrix[0,6] = Usage_by_population_matrix[0, 5] = 0.5\n",
    "print('Usage_by_population_matrix', Usage_by_population_matrix)\n",
    "\n",
    "# https://www.geeksforgeeks.org/different-ways-to-create-pandas-dataframe/\n",
    "Usage_by_population_df = pd.DataFrame(Usage_by_population_matrix,\n",
    "                                      index = Population_name,\n",
    "                                      columns = Level_name)\n",
    "print(Usage_by_population_df)\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gGfH93RCkjXk",
    "colab_type": "text"
   },
   "source": [
    "# Required Equipment n per capita per sub-population p per capita is R[p, n]\n",
    "\n",
    "This is the first multiplication where we take the two matrices and multiply them together. So this will give us a matrix. Each row is for the populations and then each column shows the daily usage by population."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "rUb-b6EPlUre",
    "colab_type": "code",
    "outputId": "bedadb14-0807-4ca2-af23-5620a1e46e6d",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    }
   },
   "source": [
    "print('Daily_usage_df', Daily_usage_df.shape)\n",
    "print('Usage_by_population_df', Usage_by_population_df.shape)\n",
    "\n",
    "# Note with Panda multiply the index of rows and the columns have to match\n",
    "Required_df = Usage_by_population_df @ Daily_usage_df\n",
    "\n",
    "print('Required_df', Required_df)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EmHR1tXJMYW0",
    "colab_type": "text"
   },
   "source": [
    "# Total Required equipment for each Population T[p, n]\n",
    "\n",
    "We are now just going to case the Population count vector across the required per capita to get the total required across all populations. So we need [element-wise multiplication](https://stackoverflow.com/questions/40034993/how-to-get-element-wise-matrix-multiplication-hadamard-product-in-numpy) which is denoted and this works because of casting, so P is duplicated for each column. Th syntax is different in each variant, for [Dataframes](https:/stackoverflow.com/questions/21022865/pandas-elementwise-multiplication-of-two-dataframes_)\n",
    "\n",
    "    # In Matlab\n",
    "    R .* P \n",
    "    # In Numpy\n",
    "    R * P\n",
    "    # In \n",
    "    R * P.values\n",
    "\n",
    "This is a pretty easy calculation, you just need the element-wise multiplication of the actual population numbers against the per-capita needs. Because of t he way broadcasting works, the vector is spread properly\n",
    "\n",
    "$T[p, n] = R[p, n] * P[p, 1].values$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "BZn5jauDNHgU",
    "colab_type": "code",
    "outputId": "45fab555-9b35-41ca-99e0-87244909d36f",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    }
   },
   "source": [
    "\n",
    "print('Required_df shape', Required_df.shape)\n",
    "print(Required_df)\n",
    "print('Population_df shape', Population_df.shape)\n",
    "print(Population_df)\n",
    "\n",
    "Total_required_df = Required_df * Population_df.values\n",
    "print(Total_required_df)\n",
    "\n",
    "# another formulation\n",
    "Total_items_per_population_df = Required_df * Population_df.values\n",
    "print('Total items for each subpopulation')\n",
    "print(Total_items_per_population_df)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QjU4CuZGYi_w",
    "colab_type": "text"
   },
   "source": [
    "# Now convert Population rows into Essential rows with E[e, p]\n",
    "This changes the labels and let's you assign each Population with an Essentiality index. Eventually, the essentiality will represent a time series. So e=0 means start at week 0 (for healthcare) and then e=N means start at week N. \n",
    "\n",
    "We can even do a time series on that too, say have a sigmoid for the starting or some other sort of lambda.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "CvE_lclnbL4q",
    "colab_type": "code",
    "outputId": "49e9e612-5fc8-432c-94ff-90af377136a2",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    }
   },
   "source": [
    "# this is a square inverse as we healthcare works are week 0 and non-healthcare is week 1\n",
    "\n",
    "Essential_name = [ \"Essential\",\n",
    "                   \"Non-essential\"]\n",
    "\n",
    "Essential_by_population_matrix = [\n",
    "                                    [1, 0],\n",
    "                                    [0, 1]                                 \n",
    "                                  ]\n",
    "\n",
    "Essential_by_population_df = pd.DataFrame(Essential_by_population_matrix,\n",
    "                                      index = Essential_name,\n",
    "                                      columns = Population_name)\n",
    "\n",
    "print('Essential by population', Essential_by_population_df)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8HzfZeTtbRFY",
    "colab_type": "text"
   },
   "source": [
    "# Now use that matrix to convert Required equipment by population to Required by Essentiality\n",
    "\n",
    "So we that T[p, n] and we convert with another matrix multiple as\n",
    "\n",
    "    RE[e, n] = E[e, p] @ T[p, n] "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LX2LPIO2clDN",
    "colab_type": "code",
    "outputId": "073e9ec3-c349-411e-d0cf-701c2d498aef",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    }
   },
   "source": [
    "Required_by_essential_df = Essential_by_population_df @ Total_required_df\n",
    "print('Require by Essential Index', Required_by_essential_df)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Hj_MLrCc7zZ",
    "colab_type": "text"
   },
   "source": [
    "# Cost matrix is the Cost per item for all items C[e, n]\n",
    "\n",
    "For each row, we get a cost for the item, so for intance, if N95 surgicals are $3 and non-ASTM disposables are $0., the the vector looks like, because of broadcasting, if you just define a single row, it will be copied against all esesential levels\n",
    "\n",
    "    CE[e, n] =[ $3.00, $0.50]\n",
    "\n",
    "This should be the same width as the Product slicing.\n",
    "\n",
    "In this simple case, all costs across all essentials are identical, but in the more sophisticated costs, costs will vary by volume, so cost becomes a different across different essential levels as volumes and purchasing power are different\n",
    "\n",
    "    CE[e, n] = [ [ $3, $0.20 ],\n",
    "                [ $5, $0.50 ]\n",
    "              ]"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_qpsY-HvdGDf",
    "colab_type": "code",
    "outputId": "dfa052fa-8be8-493a-f51f-328d81c9ac99",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    }
   },
   "source": [
    "# dataframe is a matrix\n",
    "# Assumes the same price for all users\n",
    "# Note that to make the math work, it need to a Numpy Array\n",
    "Cost_per_item_by_essential_matrix = np.array([ 3, 0.5] )\n",
    "\n",
    "# Assumes a different price depending on the essnetial level is 50% more\n",
    "Cost_per_item_by_essential_matrix = [ Cost_per_item_by_essential_matrix, Cost_per_item_by_essential_matrix * 1.5 ]\n",
    "Cost_per_item_by_essential_df = pd.DataFrame(Cost_per_item_by_essential_matrix,\n",
    "                                index = Essential_name,\n",
    "                                columns = Item_name)\n",
    "print('Cost per item',Cost_per_item_by_essential_df)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AUF3Zt2imJiA",
    "colab_type": "text"
   },
   "source": [
    "# Now calculate the costs based on Requirements and Cost matrix\n",
    "\n",
    "this is just the element-wise multiplication\n",
    "\n",
    "TE[e, n] = RE[e, n] * CE[e, n].value"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1A-BZeN-pJ-u",
    "colab_type": "code",
    "outputId": "68fb6f58-50e9-4df4-ff55-a30e4c3dfdea",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    }
   },
   "source": [
    "Total_cost_by_essential_df = Required_by_essential_df * Cost_per_item_by_essential_df.values\n",
    "print('Total cost by essential_df', Total_cost_by_essential_df )"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6dNjaRk-p87e",
    "colab_type": "text"
   },
   "source": [
    "# Finally use the Stockpile in days per essential to get the Stockpile by essential population\n",
    "\n",
    "This is another element wise multiply\n",
    "\n",
    "S[e, n] = RE[e, n] * DE[1, n].values"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "v0BcGolJqS83",
    "colab_type": "code",
    "outputId": "07fa7e4c-531f-4eba-b24a-18f287a316d9",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    }
   },
   "source": [
    "# how much stockpile per item is needed for each level\n",
    "Day_stockpile_by_essential_matrix = [ [30], \n",
    "                                     [0]]\n",
    "Day_stockpile_by_essential_df = pd.DataFrame(Day_stockpile_by_essential_matrix,\n",
    "                                             index= Essential_name)\n",
    "\n",
    "print('Day_stockpile_by essential', Day_stockpile_by_essential_df,)\n",
    "\n",
    "# use .to_numpy as clearer than .values but this does not work\n",
    "Stockpile_by_essential_df = Required_by_essential_df * Day_stockpile_by_essential_df.values\n",
    "\n",
    "print('Stockpile by essential', Stockpile_by_essential_df)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U6tdby_AHdre",
    "colab_type": "text"
   },
   "source": [
    "# Estimating the Behavior over time BM[p, t] by population\n",
    "So this is a flatten matrix for each population, what happens over time. So for instance 1.0 means the same activity level as pre-COVID-19. You can have more than 1.0 if there is a frenzy of activity as an example."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1QuIZ5y6yesE",
    "colab_type": "code",
    "outputId": "e3e2b934-cd24-4162-ecfd-1e76c787402d",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    }
   },
   "source": [
    "# An example of a hard coded matrix without parameters\n",
    "Behavioral_time_by_population_matrix = [ [ 0.5, 1, 1],\n",
    "                                         [ 0, 0, 1]]\n",
    "Period_name = [ 'Week 1', 'Week 2', 'Week 3']\n",
    "Behavioral_time_by_population_df = pd.DataFrame(Behavioral_time_by_population_matrix,\n",
    "                                                index=Population_name,\n",
    "                                                columns=Period_name)\n",
    "print('Behavioral Time Parms')\n",
    "print(Behavioral_time_by_population_df)\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jhJXjmqHzBkk",
    "colab_type": "text"
   },
   "source": [
    "# Creating a time series for Resources needed R[p, n] from BM[p, t] to Rtime[p, n, t]\n",
    "\n",
    "You can take any matrix and make it time oriented. So let's take the essential Stockpile and vary it by time.\n",
    "\n",
    "What is easiest to do with the time series feature of is to flatten all the matrices. And then it seems you just need to add a timestamp.\n",
    "\n",
    "The other apporach is to use the so called multiindex which let's you do multi dimensionals in a 2-D way.\n",
    "\n",
    "The use of iterables makes it easy to mix the labels in [multiindexing](https://pandas.pydata.org/pandas-docs/stable/user_guide/advanced.html) but this is really a display thing. You cannot easily take a multidimensional cube and do a multiply. so the plan is to convert the multi-index to a numpy tensor do the math and then put it back.\n",
    "\n",
    "But the easier way is to do this as a pure matrix with numpy thanks to [Stackoverflow](https://stackoverflow.com/questions/40644851/numpy-broadcast-multiplication-over-one-common-axis-of-two-2d-arrays) the key is something called einsum which does give you a simple way to transform\n",
    "\n",
    "    np.einsum('ij, jk -> ijk', A, B)\n",
    "    # this basically extends the A[i,j] into the k dimension which is what we want where i=p, j=n, k=t\n",
    "    # these need to be in alphabetical order \n",
    "    np.einsum('ij,ik -> ijk ', R, BM)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DThfVZtG4Z7S",
    "colab_type": "code",
    "outputId": "485d57b9-1271-426a-b9b7-b2801004af1d",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 899
    }
   },
   "source": [
    "# https://stackoverflow.com/questions/25440008/python-pandas-flatten-a-dataframe-to-a-list\n",
    "# Not the right approach what we want is a stack of R[p, n]\n",
    "# When R[p, n] * BM[p, 1] => Rt[p, n .* BM[p,1]]\n",
    "R = np.array([[ 0, 100],\n",
    "              [50, 0]])\n",
    "print('Test R', R)\n",
    "BM = np.array ([ [ 0, 1],\n",
    "                 [ 1, 1]])\n",
    "print('Test BM', BM)\n",
    "Rt=np.zeros((2,2,2))\n",
    "Rt1=np.zeros((2,2))\n",
    "print('Rt', Rt)\n",
    "# https://stackoverflow.com/questions/4455076/how-to-access-the-ith-column-of-a-numpy-multidimensional-array\n",
    "# get the column vector for time 0 note that [0] means return a column vector not a row vector\n",
    "BM0=BM[:,[0]]\n",
    "print('BM0', BM0)\n",
    "print('BM0.shape', BM0.shape)\n",
    "\n",
    "# This should broadcast so R is p xn and BM0 is p x 1\n",
    "Rt0 = R * BM0\n",
    "print('Rt[:,:,0]', Rt0)\n",
    "\n",
    "print('Total Resource by population rp x n')\n",
    "print(Total_required_df)\n",
    "print('shape', Total_required_df.shape)\n",
    "\n",
    "print('Behavorial model p x t')\n",
    "print(Behavioral_by_population_df)\n",
    "print('shape', Behavioral_by_population_df.shape)\n",
    "\n",
    "# Now the magic note the output is in alphabetical order\n",
    "# https://ajcr.net/Basic-guide-to-einsum/\n",
    "# Total_required_df i rows, j columns; Behavorial_time i row, k column\n",
    "ijk=np.einsum('ij, ik -> ijk', Total_required_df, Behavioral_time_by_population_df)\n",
    "print('ijk', ijk.shape, ijk)\n",
    "\n",
    "kji=np.einsum('ij, ik -> kji', Total_required_df, Behavioral_time_by_population_df)\n",
    "print('kji', kji.shape, kji)\n",
    "print('kji[0]', kji[0])\n",
    "\n",
    "kij=np.einsum('ij, ik -> kij', Total_required_df, Behavioral_time_by_population_df)\n",
    "print('kij', kij.shape, kij)\n",
    "print('kij[0]', kij[0])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tmzf8K60yA7a",
    "colab_type": "text"
   },
   "source": [
    "# Behavioral modeling BM[p, t] from parameters BMp[p, 2]\n",
    "This long term will be a Python function from a PIP library which will also get instantiated as a REST API so it can be called. (see the section on implementation\n",
    "\n",
    "But this implements the time series of restart across a set of populations. The interface is the Dataframe BM[p, t] which is a factor for how much activitiy there is the Population matrix. So this model says the first population, the non-healthcare workers start 40% in week 2 and then get to 60% by week 3.\n",
    "\n",
    "The healthcare workers start and this is manually encoded.\n",
    "\n",
    "For simplicity, you can use a generator function as well to create this so you don't have to create a gigantic table by hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C7NW8qt4SN6U",
    "colab_type": "text"
   },
   "source": [
    "## Using parameters to create the activity matrix\n",
    "This is harder than it looks, but the idea is to use some simple functions and generate a matrix. The main problem here is that you want to do it quickly, so ideally, you would use vector match to create all the matrices automatically as a template\n",
    "\n",
    "The first attempt with numpy.fromfunction sort of works, but you cannot parameterize them very well, so the solution is to use vectorize as a way to get it across an entrie array.\n",
    "\n",
    "But since this varries by row, instead, we use from function for each row and then can concatenate them together"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ye7uD8BvSWUE",
    "colab_type": "code",
    "outputId": "c8848a34-99f6-45b3-9a86-142e599e5fcf",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    }
   },
   "source": [
    "# How to create an model automagically with just paramaters\n",
    "# Remember Python doesn't have arrays, it uses lists for that\n",
    "# https://www.programiz.com/python-programming/matrix\n",
    "# And index syntax is completely different from numpy\n",
    "Behavioral_by_population_parameter_matrix = [ ['logistic', 1, 0.7],\n",
    "                                              ['logistic', 0, 1 ] ]\n",
    "Logistic_parameter_name = [ 'function', 'loc', 'scale']\n",
    "\n",
    "Behavioral_by_population_parameter_df = pd.DataFrame(Behavioral_by_population_parameter_matrix,\n",
    "                                                     index=Population_name,\n",
    "                                                     columns=Logistic_parameter_name)\n",
    "\n",
    "print(Behavioral_by_population_parameter_df)\n",
    "print('scipy stats')\n",
    "from scipy.stats import logistic \n",
    "for t in range(1,3):\n",
    "  print(t, logistic.cdf(t, loc=1.4, scale=2.0))\n",
    "\n",
    "# https://numpy.org/doc/stable/reference/generated/numpy.fromfunction.html\n",
    "# Can use a function to create a long list\n",
    "print(type(Behavioral_by_population_parameter_matrix))\n",
    "print(Behavioral_by_population_parameter_matrix)\n",
    "\n",
    "# remember Python matrices index from 0\n",
    "print(Behavioral_by_population_parameter_matrix[0][1],\n",
    "      type(Behavioral_by_population_parameter_matrix[0][2]))\n",
    "\n",
    "# https://stackoverflow.com/questions/9452775/converting-numpy-dtypes-to-native-python-types\n",
    "# need to turn them into real numbers\n",
    "value=Behavioral_by_population_parameter_matrix\n",
    "\n",
    "# Note that From function is not called once per cell, it is called exactly once and not once per cell\n",
    "# You need to use np.vectorize to push the function across all cell elements\n",
    "# https://stackoverflow.com/questions/18702105/parameters-to-numpys-fromfunction\n",
    "\n",
    "# https://stackoverflow.com/questions/50997928/typeerror-only-integer-scalar-arrays-can-be-converted-to-a-scalar-index-with-1d/50997969\n",
    "print(np.fromfunction(lambda p, t : logistic.cdf(t, \n",
    "                                           loc=1.2,\n",
    "                                           scale=2.3\n",
    "                                           ), (2, 3)))\n",
    "\n",
    "# This works, so you can do a lookup from a global variable inside a lambda\n",
    "print(np.fromfunction(lambda p, t : logistic.cdf(t, \n",
    "                                           loc=Behavioral_by_population_parameter_matrix[0][1],\n",
    "                                           scale=Behavioral_by_population_parameter_matrix[0][2]\n",
    "                                           ), (2, 3)))\n",
    "\n",
    "\n",
    "# now try using p and this works fine\n",
    "print(np.fromfunction(lambda p, t : logistic.cdf(t, \n",
    "                                           loc=p,\n",
    "                                           scale=p+1\n",
    "                                           ), (2, 3)))\n",
    "\n",
    "# This does not work, the error is only integer scalar arrays can be converted to a scalar index \n",
    "print('getting just p+t')\n",
    "# This works because you are actually doing array match, p is (2,3) and t is (2, 3)\n",
    "print(np.fromfunction(lambda p, t : p+t, (2, 3)))\n",
    "print('value of p and t')\n",
    "print(np.fromfunction(lambda p, t : p, (2, 3)))\n",
    "print(np.fromfunction(lambda p, t : t, (2, 3)))\n",
    "\n",
    "# https://stackoverflow.com/questions/18702105/parameters-to-numpys-fromfunction\n",
    "# Explains what is going on \n",
    "\n",
    "print('type of p and t')\n",
    "# Unintuitive, what you get is not a scalar in p, but a numpy array when in the above you\n",
    "# just get a scalar, why is this?\n",
    "# The function actually gets an input that is the shape noted so it is (2,3)\n",
    "# https://numpy.org/doc/stable/reference/generated/numpy.fromfunction.html\n",
    "print(np.fromfunction(lambda p, t : type(p), (2, 3)))\n",
    "print(np.fromfunction(lambda p, t : type(t), (2, 3)))\n",
    "\n",
    "# This demonstrates that what you get is every possible point\n",
    "# so p will be all the row numbers which are the same across all the columns\n",
    "# and t will be the column indices which are the same across all columns\n",
    "# which is not what stackoverflow said\n",
    "# So to use this, you need a function which does array math properly\n",
    "def show(p, t):\n",
    "  print('show')\n",
    "  print('called value of p')\n",
    "  print(type(p), p.shape, p)\n",
    "\n",
    "  print('called value of t')\n",
    "  print(type(t), t.shape, t)\n",
    "\n",
    "print(np.fromfunction(show, (3, 3)))\n",
    "\n",
    "print('shapes of p and t')\n",
    "print(np.fromfunction(lambda p, t : p.shape, (2, 3)))\n",
    "print(np.fromfunction(lambda p, t : t.shape, (2, 3)))\n",
    "\n",
    "# This works because logistics handles arrays, so takes all of t and applies it\n",
    "# The problem is that loc and scale need scalars, so you can index it with p, which is a matrix\n",
    "print(np.fromfunction(lambda p, t : logistic.cdf(t, \n",
    "                                           loc=Behavioral_by_population_parameter_matrix[0][1],\n",
    "                                           scale=Behavioral_by_population_parameter_matrix[0][2]\n",
    "                                           ), (2, 3)))\n",
    "\n",
    "# This will not work because you are getting p as an array and t as an array and it expects\n",
    "# a array function\n",
    "# print(np.fromfunction(lambda p, t : logistic.cdf(t, \n",
    "#                                           loc=Behavioral_by_population_parameter_matrix[p][1],\n",
    "#                                           scale=Behavioral_by_population_parameter_matrix[0][2]\n",
    "#                                           ), (2, 3)))\n",
    "\n",
    "# The right approach is to just use the working code above but apply it to each row and then glom the whole thing together\n",
    "# https://www.pluralsight.com/guides/numpy-arrays-iterating\n",
    "# This works because numpy treats a matrix actually as a collection of lists so very elegant\n",
    "print(Behavioral_by_population_df)\n",
    "print(Behavioral_by_population_parameter_matrix)\n",
    "\n",
    "# Iterating over a 2-D dataframe which is much more complicated than a matrix\n",
    "# https://stackoverflow.com/questions/16476924/how-to-iterate-over-rows-in-a-dataframe-in-pandas\n",
    "# https://www.geeksforgeeks.org/different-ways-to-iterate-over-rows-in-pandas-dataframe/\n",
    "# note that in Dataframes, the rows are actually behind, so it is column major\n",
    "print('iterate over rows in dataframe')\n",
    "for population in Behavioral_by_population_df.index:\n",
    "  print('population', type(population), population)\n",
    "  print(Behavioral_by_population_df['Week 1'][population])\n",
    "\n",
    "print('iterate using iterrows')\n",
    "for index, population in Behavioral_by_population_df.iterrows():\n",
    "  print(index, type(population), population['Week 1'])\n",
    "\n",
    "print('iterate of rows in numpy matrix')\n",
    "for population in Behavioral_by_population_matrix:\n",
    "  # you will get each item overall\n",
    "  print(population)\n",
    "\n",
    "# Another approach is apply_along_axis which is really cool\n",
    "# https://stackoverflow.com/questions/16468717/iterating-over-numpy-matrix-rows-to-apply-a-function-each\n",
    "# \n",
    "\n",
    "# https://stackoverflow.com/questions/40644851/numpy-broadcast-multiplication-over-one-common-axis-of-two-2d-arrays\n",
    "# https://het.as.utexas.edu/HET/Software/Numpy/reference/generated/numpy.outer.html\n",
    "# https://stackoverflow.com/questions/26089893/understanding-numpys-einsum\n",
    "# A completely different approach\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XhXkKFfuW1Gy",
    "colab_type": "text"
   },
   "source": [
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jggktC4lTNrs",
    "colab_type": "text"
   },
   "source": [
    "# Attachment: Test Code\n",
    "\n",
    "Used to test various features of the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H1uq0RMITXy3",
    "colab_type": "text"
   },
   "source": [
    "## Test of cloning an external Repo\n",
    "\n",
    "NOte that this does a complete clone in the virtual machine, make sure you have enough space. Also you need to reclone when you close a Notebook instance, so this can be slow with lots of data.\n",
    "\n",
    "However, it does allow you checkout particular branches and have a realiable dataset."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kD9qp04tU9-L",
    "colab_type": "code",
    "outputId": "18fd7c93-5c8f-4f02-dec1-4fe0fd4fbc7b",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 199
    }
   },
   "source": [
    "# Clone the entire repo.\n",
    "!git clone -l -s git://github.com/jakevdp/PythonDataScienceHandbook.git cloned-repo\n",
    "%cd cloned-repo\n",
    "!ls"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MDnksm1MTmV7",
    "colab_type": "text"
   },
   "source": [
    "## Test of copying a single file from a repo\n",
    "\n",
    "This one way to get small datasets, you just point to the raw file and use `!curl` to bring it into the machine."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Q7eqOYtkU99S",
    "colab_type": "code",
    "outputId": "77f88b21-0951-42fa-c3bb-ee2d6d31e8ec",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    }
   },
   "source": [
    "# Fetch a single <1MB file using the raw GitHub URL.\n",
    "!curl --remote-name \\\n",
    "     -H 'Accept: application/vnd.github.v3.raw' \\\n",
    "     --location https://api.github.com/repos/jakevdp/PythonDataScienceHandbook/contents/notebooks/data/california_cities.csv"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "38KeztwBajb8",
    "colab_type": "text"
   },
   "source": [
    "## Test of connecting to Google Drive\n",
    "\n",
    "This we can use if we don't need a repo, but are just loading a static file. We normally want everything from a repo or reliable storage, but this is good for quick analysis. In most cases, you should just check this into a repo and then use the github raw extract instead so you get version control.\n",
    "\n",
    "Note that this does require an authentication everytime you start the Notebook, so the raw extract works better particularly if there it is a public repo."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "pKyKRJeoHtJ_",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/gdrive')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "S2ldUhSATDRY",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "with open('/gdrive/My Drive/foo.txt', 'w') as f:\n",
    "  f.write('Hello Google Drive!')\n",
    "!cat '/gdrive/My Drive/foo.txt'"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gXxIvzREazRq",
    "colab_type": "text"
   },
   "source": [
    "## Connecting two cells together for summaries with Cross-output Communications\n",
    "\n",
    "This is the best method for connecting the longer analysis to a cell that just has the executive summary data. _This does not appear to be working. Need to debug_"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ndtWkGTjlL70",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "%%javascript\n",
    "const listenerChannel = new BroadcastChannel('channel');\n",
    "listenerChannel.onmessage = (msg) => {\n",
    "  const div = document.createElement('div');\n",
    "  div.textContent = msg.data;\n",
    "  document.body.appendChild(div);\n",
    "};"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kmwMtzEslL7J",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "%%javascript\n",
    "const senderChannel = new BroadcastChannel('channel');\n",
    "senderChannel.postMessage('Hello world!');"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7AulXwNtb5yd",
    "colab_type": "text"
   },
   "source": [
    "## Creating forms for entry\n",
    "\n",
    "This is going to be used to parameterize models. This sets global variables that can be used in cells farther down."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lXWX8aG7lWvD",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "#@title Example form fields\n",
    "#@markdown Forms support many types of fields.\n",
    "\n",
    "no_type_checking = ''  #@param\n",
    "string_type = 'example'  #@param {type: \"string\"}\n",
    "slider_value = 142  #@param {type: \"slider\", min: 100, max: 200}\n",
    "number = 102  #@param {type: \"number\"}\n",
    "date = '2010-11-05'  #@param {type: \"date\"}\n",
    "pick_me = \"monday\"  #@param ['monday', 'tuesday', 'wednesday', 'thursday']\n",
    "select_or_input = \"apples\" #@param [\"apples\", \"bananas\", \"oranges\"] {allow-input: true}\n",
    "#@markdown ---\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GDR12BB4pPd1",
    "colab_type": "text"
   },
   "source": [
    "## Display Pandas data dataframes use Vega datasets as an example\n",
    "\n",
    "This uses the extension `google.colab.data_table` and there is a default data set called `vega_datasets` where you can extract data. It is not clear where the data is or how to figure out how ot use it. Google-fu does not help although the [source code](https://github.com/googlecolab/colabtools/blob/master/google/colab/data_table.py) tells us that `vega_dataset` has airport data in it.\n",
    "\n",
    "But the hing is in the name Vega which is a visualization package and [Vega Datasets access from Python](https://github.com/jakevdp/vega_datasets) are a standard set of data for visualization testing. The core datasets are kept in [github.io](https://vega.github.io/vega-datasets/)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wW43_ntJpdVh",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "%load_ext google.colab.data_table"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "knd4KPzcpdUf",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "from vega_datasets import data\n",
    "data.cars()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KwkagE3vpdTY",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "%unload_ext google.colab.data_table"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OKvLbYFyy8J-",
    "colab_type": "text"
   },
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kgrbax2npdRf",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "data.stocks()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8evpmsNgsOaF",
    "colab_type": "text"
   },
   "source": [
    "## Github Rendering of Jupyter\n",
    "\n",
    "This is pretty cool, but [Github](https://help.github.com/en/github/managing-files-in-a-repository/working-with-jupyter-notebook-files-on-github) actually renders the Jupyter notebooks as statis HTML when you browse it. That means just clicking on a `.ipynb` will give you something reasonable. It is not interactive nor is anything running behind it, but it does mean that documents produced by use are easily readable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-RV8Y3kpyklo",
    "colab_type": "text"
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "CmBj6sJOykQe",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    ""
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}
