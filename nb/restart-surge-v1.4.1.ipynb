{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Restart - NYC - 2020-06-04.ipynb",
   "provenance": [],
   "collapsed_sections": [
    "H1uq0RMITXy3",
    "MDnksm1MTmV7",
    "38KeztwBajb8",
    "gXxIvzREazRq",
    "7AulXwNtb5yd",
    "GDR12BB4pPd1"
   ],
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "",
   "display_name": ""
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/restartus/covid-projection/blob/rich-dev/Restart_NYC_2020_06_04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UPP19bJbIKev",
    "colab_type": "text"
   },
   "source": [
    "---\n",
    "\n",
    "_Proprietary and Confidential. Do not distribute without permission._\n",
    "\n",
    "---\n",
    "\n",
    "# Restart Partners  \n",
    "\n",
    "---\n",
    "_To:_ Jun Amora (Mayors Office, City of New York)  \n",
    "_From:_ Bharat Shyam, Rich Tong (Restart)  \n",
    "_Re:_ Analysis for NYC PPE needs  \n",
    "_Date:_ 20 May 2020  \n",
    "\n",
    "--- \n",
    "\n",
    "New York City needs a 90-day stockpile for the heathcare workers, first responders and congregate care facilities is really important, but coming up with an estimate for this is difficult given the variability of the infection and the uncertainty in the degree of economic recovery and social mobility. Therefore, we are providing another resource model to augment yours that shows that our figures are within 30%-50% of your bottoms estimate. Given that we are happy to:\n",
    "\n",
    "- _Refine healthcare estimates_. All models are heavily dependent on estimates of population involved and usage data. \n",
    "- _Non-healthcare estimates_. For instance, this model does project needs outside of the healthcare area such as small business, vertical industries and vulnerable populations.\n",
    "- _Long-term modeling_. We are extending the model to include test equipment, disinfectant wipes and liquid disinfectants, so happy to add things that you need. Also we will be integrating epi and economic models too and would love to partner with you on that.\n",
    "\n",
    "Given the uncertainties involved, this might help you make the right estimates. What follows next are:\n",
    "\n",
    "1. Disclaimer. This is not a definitive estimate. You should use other sources and information to make your decisions.\n",
    "2. Data Sources. We have included the model source data, how the model is constructed and then results. Feel free to use this data and modify as appropriate, but it serves as documentation for all the assumptions made.\n",
    "3. Model. The way the calculation is done with assumptions and resulting projections\n",
    "4. Outputs. The conclusions we can draw from the projection.\n",
    "\n",
    "## Disclaimer\n",
    "It must be noted that the Restart Partners (\"Restart\") Equipment Model (the \"Model\") is made available for public use free of charge. Determining equipment needs for each jurisdiction, entity or other party (each a \"User\") is a complex and multifaceted decision process. Restart does not does not have the authority or ability to assign empirical risks levels nor make definitive use decisions for any User. Rather, the Model provides one approach to making recommendations that can help Users make decisions about their potential equipment uses by allowing them to calculate their potential requirements. Users are strongly encouraged to consider other sources of information and expressly disclaim any cause of action they may have against Restart arising from or relating to the Model or its analysis. Implementing the equipment levels projected by the Model will not eliminate the risk of COID-19 cases being linked to activites in an economy or workplace. In this context, it is important to note that this equipment alone will not eliminate the risk of infection. All Users should remain informed about and abide by any decisions made by local public health and government authorities regarding specific mitigation efforts, including equipment in the model, as the situation is dynamic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Xo1YI-GvU4o",
    "colab_type": "text"
   },
   "source": [
    "# Model Data\n",
    "\n",
    "Because we do not have New York City specific data, we used various open data sources to fill in the five major assumptions in the model:\n",
    "\n",
    "1. Usage by Population. This cuts the item usage per person per day. This right now is a series of levels. So we have four levels for civilians and then two levels for healthcare workers.\n",
    "2. Usage per Patient. This is the way Epidemiological models work. That is, given a number of patients, calculate how much they will need. The model currently uses the [WHO Surge Essential Supplies Forecasting Tool v1.2](https://www.who.int/emergencies/diseases/novel-coronavirus-2019/technical-guidance/covid-19-critical-items) and estimates the entire US population use with 1,000 cases and fast transmission and slow response. So this is a very pessimistic scenario. This makes sense when calculating the surge estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DOPqqurOE8-E",
    "colab_type": "text"
   },
   "source": [
    "# Strategy for the Model\n",
    "\n",
    "The next steps are a little complicated, but we are converting the entire model into a series of vectorized operations. \n",
    "\n",
    "## Daily Usage of Equipment by Protection levels D[l, n]\n",
    "\n",
    "So first we need the inputs which are the list of protection levels p x the number of items we are tracking n. So this is an p x n matrix where each entry says for protection level l, we have so many items per capita per day. So the first data list is Daily_usage D = l rows x n columns or D.shape = ( l, n )\n",
    "\n",
    "In Python speak this is Usage_pd since it is a _P_anda _D_ataframe. Right now this is a test matrix that is constructed in the Jupyter Notebook, but longer term, it should be pulled from an database with the suitable annotations.\n",
    "\n",
    "## Sub-Population Count vector P[p, 1]\n",
    "\n",
    "In the original model, we had p sub-populations. In the simplest model, it was just two populations: non-employees of healthcare companies and employees. With SOC and other codes, there are close to a thousands. So the populations are a vector that includes a description and a population count.\n",
    "\n",
    "So for example P = [ 7400, 435 ] which is just about the right numbers for Seattle and is a [ 1, p ] vector\n",
    "\n",
    "## Sub-Population Usage of Protection Levels U[p, l]\n",
    "\n",
    "You can think of this as a one-hot matrix in the most simple form. That is for each subpopulation, what level of protection do you need. \n",
    "\n",
    "For example, in the simplest case, if there are six protection levels, then if non-employees get level 1 and healthcare employees get level 6, then the matrix looks like:\n",
    "\n",
    "     0 1 0 0 0 0 0\n",
    "     0 0 0 0 0 0 1\n",
    "\n",
    "While date entry is complicated, this let's you take any given population and give it fractions of protection. For instance, if a healthcare employer typically had 50% of it's workers as office workers at level 2, 25% as customer facing and 10% taking care of non COVID and 15% in direct contact, that vector would look like:\n",
    "\n",
    "    0 0.5 0.25 0 0 0.10 0.15\n",
    "\n",
    "This gives the modeler great flexibility with employers or any population\n",
    "\n",
    "The matrix of usage U looks like p rows and l columns\n",
    "\n",
    "    Usage_pd.shape = [ p, l ]\n",
    "\n",
    "## Required Equipment per capita per sub-population R[p, n]\n",
    "\n",
    "So with this, you can see that with a single operation you can get to the actual equipment levels require per person per day for a given population. Note that there is new Python 3.5 syntax for [matrix multiply](https://docs.python.org/3/whatsnew/3.5.html#whatsnew-pep-465)\n",
    "\n",
    "    U x D = [ p, l ] x [l, n] = R[p, n]\n",
    "    # in the new Python 3.5 syntax using ampersand\n",
    "    # np.dot for matrices but not tensors\n",
    "    R = U @ D\n",
    "    R = U.dot(D)\n",
    "\n",
    "In Python Numpy speak, we are doing a matrix [multiply](https://www.tutorialexample.com/understand-numpy-np-multiply-np-dot-and-operation-a-beginner-guide-numpy-tutorial/)\n",
    "\n",
    "## Total required equipment for a sub-population T[p, n]\n",
    "\n",
    "Now that we have the per-capita requirements, we need to do a scalar multiply by row\n",
    "\n",
    "    R[p, n] x P[p, 1] = T[p, n]\n",
    "    # Or in python using broadcasting which extends P out n columns\n",
    "    T = R * P\n",
    "\n",
    "## Merging populations into fewer buckets with Essential index by population E[e, p]\n",
    "\n",
    "Many times the subpopulations are going to be too large to understand. For instance when there are 800 job classfied by SOC or where there are 350 employer class by NAICS-6, so for convenience, we define essential levels. You can think of the of this as for each population, where do they fit in where they start. Essential (which has changed since version 1.x) can be thought of as the time period of start. So Essential 0 (like Defcon 1), is the most important and so forth. \n",
    "\n",
    "This let's you stage start up, so for the example subpopulations of non-healthcare employed and heathcare employed, it might look like a simple matrix across 6 start periods as or more analytically E is e rows and p columns.\n",
    "\n",
    "So in the example, it says the first population starts at time 0 and then the second starts in week 6\n",
    "\n",
    "    1 0\n",
    "    0 0\n",
    "    0 0\n",
    "    0 0\n",
    "    0 0\n",
    "    0 0\n",
    "    0 1\n",
    "\n",
    "But this system also allows a stageed restart, so for example, if you want have the workers to come back in the next period for healthcare employees and this series could even be generated as a lambda with any arbitrary function, so in this example, population 1 starts 50% in week 0, then slows continues. \n",
    "\n",
    "While population 2 doesn't start until week 4 and tails up\n",
    "\n",
    "    0.5 0\n",
    "    0.4 0\n",
    "    0.1 0\n",
    "    0 0\n",
    "    0 0.2\n",
    "    0 0.4\n",
    "    0 0.6\n",
    "\n",
    "## Requirements by essential index Re[e, n]\n",
    "\n",
    "In some sense we are doing compression by this, so we are looking at Essential index e is much less than the number of populations p. Or more succinctly e << p and we can get to E with a transpose\n",
    "\n",
    "    E[e, p] x T[p, n] = e x p * p x n = R[e, n]\n",
    "    # In python this looks like\n",
    "    Re = E @ T\n",
    "\n",
    "## Then there is a Cost per item per essential row\n",
    "\n",
    "For simplicity we can assume that each essential level has different costs. In reality, the costs will actually be more complicate and C[p, n] which is much more complicate.d\n",
    "\n",
    "For each item, what is the cost for N95 is $3 and non-surgical is $0.50\n",
    "    C[e, n] = [ $3, $0.50 ]\n",
    "\n",
    "## The easy parts Required Cost per day RC[e, n] and Stockpile S[e, n]\n",
    "\n",
    "OK that was the hard stuff, with these matrices reduced to essential levels and the equipment needed for each, there are just some simple scalar multiplies to get where Cost is a row vector that is all the costsj which is a element-wide multiplication.\n",
    "\n",
    "## Then the Stockpile need by essential levels is SE[e,1]\n",
    "\n",
    "So for each essential you need a different stockpile. Usually more essential needs more levels\n",
    "\n",
    "## Gross cost for equipment by level and stockpile by essential\n",
    "\n",
    "So both the cross cost and the stockpile are done by level as a element-wide multiplication.\n",
    "\n",
    "    Gross cost for the equipment = RC[e, n] = R[e, n] * C[e, n].value\n",
    "    Stockpile needed for d Days = S[e, n] = R[e, n] * SE[e, 1].value\n",
    "\n",
    "Obviously you may not want to stock pile for all e Essential levels, so you just select what you want for instance S[0] will give you the stockpile needs for the most essential level 0.\n",
    "\n",
    "## Handling disinfection, goggle types and reuse\n",
    "\n",
    "We can handle any arbitrary vector of items, just change the list. Longer term, we will have our own GUID system, but right no rely on unique text strings\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_o_lYYWBgOsH",
    "colab_type": "text"
   },
   "source": [
    "# Detail of Model\n",
    "\n",
    "These are the details of the model. It is a good example of the parameters that you will need to add. Make sure that you have good advice from medical authorities when looking over these parameters"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TU4PflJ5b2zK",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Get libraries\n",
    "import pandas as pd\n",
    "import numpy as np"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FPZYjkTlg8CS",
    "colab_type": "code",
    "cellView": "form",
    "colab": {}
   },
   "source": [
    "# https://colab.research.google.com/notebooks/forms.ipynb#scrollTo=ZCEBZPwUDGOg\n",
    "#@title Basic Model Parameters\n",
    "#@markdown ####Enter Model Description here:\n",
    "\n",
    "model_name = 'NYC Surge Forecast'  #@param\n",
    "model_description = 'v1.4 WHO Surge'  #@param {type: \"string\"}\n",
    "recurrence_index = 45  #@param {type: \"slider\", min: 0, max: 100}\n",
    "recovery_index = 62  #@param {type: \"slider\", min: 0, max: 100}\n",
    "revision =   103#@param {type: \"number\"}\n",
    "date = '2020-05-20'  #@param {type: \"date\"}\n",
    "model_type = \"surge\"  #@param ['surge', '3-month', '6-month']\n",
    "\n",
    "#@markdown ---\n",
    "#@markdown ####Daily Usage of Equipment Per Person\n",
    "units = \"10,000\" #@param [\"1,000,000\", \"100,000\", \"10,000\", \"1,000\"] {allow-input: true}\n",
    "\n",
    "#@markdown ---"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yLFSWaOVi7tR",
    "colab_type": "text"
   },
   "source": [
    "## Daily Usage of Equipment __n__ by Protection levels __l__ is D[l, n]"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1xwe8g08yRbG",
    "colab_type": "code",
    "outputId": "713e30a4-16b7-4ad4-d7ef-8a363611bb5b",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    }
   },
   "source": [
    "# Eventually we will do this from a database import, but for now, let's use\n",
    "# the data that is normally in the Excel sheet and just recreate \n",
    "# https://colab.research.google.com/drive/1Bcx54NQePYt88RWWmODrRA1pxz-2tnNW?authuser=5#scrollTo=1xwe8g08yRbG\n",
    "\n",
    "# Using PEP https://www.python.org/dev/peps/pep-0008/\n",
    "# For simplicity do as a dictionary\n",
    "Item_name = [\n",
    "              'N95 Surgical Respirator',\n",
    "              'N95 Mask',\n",
    "              'ASTM 3 Surgical Mask',\n",
    "              'ASTM 1-2 Surgical Mask',\n",
    "              'Non-ASTM Mask'\n",
    "              'Reusable Cotton Mask'\n",
    "              'Cotton Mask with Ear Loop',\n",
    "              'Face Shield',\n",
    "              'Goggles'\n",
    "              'Gown',\n",
    "              'Gloves',\n",
    "              'Shoe Covers',\n",
    "              'Test Kits',\n",
    "              'Disinfectant (30ml)',\n",
    "              'Disinfectant wipes'\n",
    "            ]\n",
    "\n",
    "# For this demo, we will just test with two\n",
    "Level_name = [ 'WA0', 'WA1', 'WA2', 'WA3', 'WA4', 'WA5', 'WA6']\n",
    "Item_name = [ 'N95 Surgical', 'non ASTM Mask']\n",
    "print('Item_names', Item_name)\n",
    "Daily_usage_matrix = [\n",
    "                [ 0, 0 ],\n",
    "                [ 0, 1 ],\n",
    "                [ 0, 2 ],\n",
    "                [ 0.1, 3],\n",
    "                [ 0.2, 4],\n",
    "                [ 0.3, 6],\n",
    "                [ 1.18, 0]]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fFHD-0I8JU8N",
    "colab_type": "text"
   },
   "source": [
    "### Daily Usage Matrix verification and conversion to Dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2uM7R8IXJTQE",
    "colab_type": "code",
    "outputId": "31d6f633-5c87-4bf2-ae91-18780a24da09",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    }
   },
   "source": [
    "print('Daily_usage_matrix', Daily_usage_matrix)\n",
    "\n",
    "Daily_usage_df = pd.DataFrame(Daily_usage_matrix,\n",
    "                              columns = Item_name,\n",
    "                              index = Level_name)\n",
    "\n",
    "# use these counts to check the matrix vector bugs\n",
    "level_count = Daily_usage_df.shape[0]\n",
    "item_count = Daily_usage_df.shape[1]\n",
    "print('usage_pd shape is ', Daily_usage_df.shape,\n",
    "      'protection level count is ', level_count,\n",
    "      'item count is ', item_count)\n",
    "\n",
    "print('Daily_usage_pd', Daily_usage_df)\n",
    "\n",
    "Daily_N95s_usage = Daily_usage_df['N95 Surgical']\n",
    "print('Daily N95 Surgical Usage', Daily_N95s_usage)\n",
    "\n",
    "# https://stackoverflow.com/questions/13187778/convert-pandas-dataframe-to-numpy-array\n",
    "print('Daily usage value in Dataframe', Daily_usage_df.values)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p4uMRbROcqpf",
    "colab_type": "text"
   },
   "source": [
    "## Population Data by sub-populations p is P[p, 1]\n",
    "\n",
    "Start with the simplest assumption, two populations, one that is `WA6` and one that is `WA2` as an example. But we will insert more data later once we decide the data source."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "odyDmPbkc3l2",
    "colab_type": "code",
    "outputId": "35e6db08-b87c-415f-f64b-78ced00e443e",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    }
   },
   "source": [
    "# This is a dummy test case, later we will use extraction first form a\n",
    "# spreadsheet and then eventually from a data store that is reliable\n",
    "# And which has revision control\n",
    "\n",
    "Population_name = ['Total less healthcare employees', 'Employees of healthcare companies']\n",
    "Population_data = [7179.6, 735.2 ]\n",
    "\n",
    "print('Population Data', Population_data)\n",
    "\n",
    "Population_df = pd.DataFrame(Population_data, index = Population_name, columns = ['Population'])\n",
    "population_count = Population_df.shape[0]\n",
    "print('population count p', population_count)\n",
    "print(Population_df)\n",
    "\n",
    "# https://note.nkmk.me/en/python-type-isinstance/\n",
    "print('type of Population_name', type(Population_name))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hqG2MmDelJZs",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    ""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mxOgyfxTet7N",
    "colab_type": "text"
   },
   "source": [
    "# Usage of PPE by Sub-population p is U[p, l]\n",
    "\n",
    "Now we have a vector which are the population usages and we have a list of needs, so we need to do a matrix multiply of population by needs. Each entry is the percentage of a population at a given level.\n",
    "\n",
    "So in this example, 50% of healthcare workers are level 5 and 50% are at level 6"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "NA65fr95e8w0",
    "colab_type": "code",
    "outputId": "07e858ac-b605-411c-853b-b0db6edfb09b",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    }
   },
   "source": [
    "# Now we need a matrix which is the pop_type x usage_type and the coefficient is just how much is needed for each\n",
    "# Do this for simplicity start with a zero matrix, we will actually load the data\n",
    "\n",
    "Usage_by_population_matrix = np.zeros([population_count, level_count])\n",
    "\n",
    "Usage_by_population_matrix[0,1] = 1.0\n",
    "Usage_by_population_matrix[1,6] = Usage_by_population_matrix[1, 5] = 0.5\n",
    "print('Usage_by_population_matrix', Usage_by_population_matrix)\n",
    "\n",
    "# https://www.geeksforgeeks.org/different-ways-to-create-pandas-dataframe/\n",
    "Usage_by_population_df = pd.DataFrame(Usage_by_population_matrix,\n",
    "                                      index = Population_name,\n",
    "                                      columns = Level_name)\n",
    "print(Usage_by_population_df)\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gGfH93RCkjXk",
    "colab_type": "text"
   },
   "source": [
    "# Required Equipment n per capita per sub-population p per capita is R[p, n]\n",
    "\n",
    "This is the first multiplication where we take the two matrices and multiply them together. So this will give us a matrix. Each row is for the populations and then each column shows the daily usage by population."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "rUb-b6EPlUre",
    "colab_type": "code",
    "outputId": "b34b801b-7141-422f-aa30-e9d894d34608",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    }
   },
   "source": [
    "print('Daily_usage_df', Daily_usage_df.shape)\n",
    "print('Usage_by_population_df', Usage_by_population_df.shape)\n",
    "\n",
    "# Note with Panda multiply the index of rows and the columns have to match\n",
    "Required_df = Usage_by_population_df @ Daily_usage_df\n",
    "\n",
    "print('Required_df', Required_df)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EmHR1tXJMYW0",
    "colab_type": "text"
   },
   "source": [
    "# Total Required equipment for each Population T[p, n]\n",
    "\n",
    "We are now just going to case the Population count vector across the required per capita to get the total required across all populations. So we need [element-wise multiplication](https://stackoverflow.com/questions/40034993/how-to-get-element-wise-matrix-multiplication-hadamard-product-in-numpy) which is denoted and this works because of casting, so P is duplicated for each column. Th syntax is different in each variant, for [Dataframes](https:/stackoverflow.com/questions/21022865/pandas-elementwise-multiplication-of-two-dataframes_)\n",
    "\n",
    "    # In Matlab\n",
    "    R .* P \n",
    "    # In Numpy\n",
    "    R * P\n",
    "    # In \n",
    "    R * P.values\n",
    "\n",
    "This is a pretty easy calculation, you just need the element-wise multiplication of the actual population numbers against the per-capita needs. Because of t he way broadcasting works, the vector is spread properly\n",
    "\n",
    "$T[p, n] = R[p, n] * P[p, 1].values$"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "BZn5jauDNHgU",
    "colab_type": "code",
    "outputId": "f2301914-40e4-4147-c64f-ac40e8800ad6",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 287
    }
   },
   "source": [
    "\n",
    "print('Required_df shape', Required_df.shape)\n",
    "print(Required_df)\n",
    "print('Population_df shape', Population_df.shape)\n",
    "print(Population_df)\n",
    "\n",
    "Total_required_df = Required_df * Population_df.values\n",
    "print(Total_required_df)\n",
    "\n",
    "# another formulation\n",
    "Total_items_per_population_df = Required_df * Population_df.values\n",
    "print('Total items for each subpopulation')\n",
    "print(Total_items_per_population_df)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QjU4CuZGYi_w",
    "colab_type": "text"
   },
   "source": [
    "# Now convert Population rows into Essential rows with E[e, p]\n",
    "This changes the labels and let's you assign each Population with an Essentiality index. Eventually, the essentiality will represent a time series. So e=0 means start at week 0 (for healthcare) and then e=N means start at week N. \n",
    "\n",
    "We can even do a time series on that too, say have a sigmoid for the starting or some other sort of lambda.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "CvE_lclnbL4q",
    "colab_type": "code",
    "outputId": "c33f74e4-7d5b-4841-d388-f3bf7cc38d3a",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    }
   },
   "source": [
    "# this is a square inverse as we healthcare works are week 0 and non-healthcare is week 1\n",
    "\n",
    "Essential_name = [ \"Essential\",\n",
    "                   \"Non-essential\"]\n",
    "\n",
    "Essential_by_population_matrix = [\n",
    "                                    [0, 1],\n",
    "                                    [1, 0]                                 \n",
    "                                  ]\n",
    "\n",
    "Essential_by_population_df = pd.DataFrame(Essential_by_population_matrix,\n",
    "                                      index = Essential_name,\n",
    "                                      columns = Population_name)\n",
    "\n",
    "print('Essential by population', Essential_by_population_df)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8HzfZeTtbRFY",
    "colab_type": "text"
   },
   "source": [
    "# Now use that matrix to convert Required equipment by population to Required by Essentiality\n",
    "\n",
    "So we that T[p, n] and we convert with another matrix multiple as\n",
    "\n",
    "    RE[e, n] = E[e, p] @ T[p, n] "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LX2LPIO2clDN",
    "colab_type": "code",
    "outputId": "8f96dbcb-0145-462d-d2c3-b897b21232a8",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    }
   },
   "source": [
    "Required_by_essential_df = Essential_by_population_df @ Total_required_df\n",
    "print('Require by Essential Index', Required_by_essential_df)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Hj_MLrCc7zZ",
    "colab_type": "text"
   },
   "source": [
    "# Cost matrix is the Cost per item for all items C[e, n]\n",
    "\n",
    "For each row, we get a cost for the item, so for intance, if N95 surgicals are $3 and non-ASTM disposables are $0., the the vector looks like, because of broadcasting, if you just define a single row, it will be copied against all esesential levels\n",
    "\n",
    "    CE[e, n] =[ $3.00, $0.50]\n",
    "\n",
    "This should be the same width as the Product slicing.\n",
    "\n",
    "In this simple case, all costs across all essentials are identical, but in the more sophisticated costs, costs will vary by volume, so cost becomes a different across different essential levels as volumes and purchasing power are different\n",
    "\n",
    "    CE[e, n] = [ [ $3, $0.20 ],\n",
    "                [ $5, $0.50 ]\n",
    "              ]"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_qpsY-HvdGDf",
    "colab_type": "code",
    "outputId": "fbaaf4d4-b09a-499c-93a0-745f463977cd",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    }
   },
   "source": [
    "# dataframe is a matrix\n",
    "# Assumes the same price for all users\n",
    "# Note that to make the math work, it need to a Numpy Array\n",
    "Cost_per_item_by_essential_matrix = np.array([ 3, 0.5] )\n",
    "\n",
    "# Assumes a different price depending on the essnetial level is 50% more\n",
    "Cost_per_item_by_essential_matrix = [ Cost_per_item_by_essential_matrix, Cost_per_item_by_essential_matrix * 1.5 ]\n",
    "Cost_per_item_by_essential_df = pd.DataFrame(Cost_per_item_by_essential_matrix,\n",
    "                                index = Essential_name,\n",
    "                                columns = Item_name)\n",
    "print('Cost per item',Cost_per_item_by_essential_df)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AUF3Zt2imJiA",
    "colab_type": "text"
   },
   "source": [
    "# Now calculate the costs based on Requirements and Cost matrix\n",
    "\n",
    "this is just the element-wise multiplication\n",
    "\n",
    "TE[e, n] = RE[e, n] * CE[e, n].value"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1A-BZeN-pJ-u",
    "colab_type": "code",
    "outputId": "25e59beb-9ad9-4538-b202-8f72a7e12959",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    }
   },
   "source": [
    "Total_cost_by_essential_df = Required_by_essential_df * Cost_per_item_by_essential_df.values\n",
    "print('Total cost by essential_df', Total_cost_by_essential_df )"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6dNjaRk-p87e",
    "colab_type": "text"
   },
   "source": [
    "# Finally use the Stockpile in days per essential to get the Stockpile by essential population\n",
    "\n",
    "This is another element wise multiply\n",
    "\n",
    "S[e, n] = RE[e, n] * DE[1, n].values"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "v0BcGolJqS83",
    "colab_type": "code",
    "outputId": "fcfe96b4-235f-4617-da4a-eda573f2331e",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    }
   },
   "source": [
    "# how much stockpile per item is needed for each level\n",
    "Day_stockpile_by_essential_matrix = [ [30], \n",
    "                                     [0]]\n",
    "Day_stockpile_by_essential_df = pd.DataFrame(Day_stockpile_by_essential_matrix,\n",
    "                                             index= Essential_name)\n",
    "\n",
    "print('Day_stockpile_by essential', Day_stockpile_by_essential_df,)\n",
    "\n",
    "# use .to_numpy as clearer than .values but this does not work\n",
    "Stockpile_by_essential_df = Required_by_essential_df * Day_stockpile_by_essential_df.values\n",
    "\n",
    "print('Stockpile by essential', Stockpile_by_essential_df)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jhJXjmqHzBkk",
    "colab_type": "text"
   },
   "source": [
    "## 3D Stack by Time using Multi-index Pandas and Numpy Tensors\n",
    "\n",
    "You can take any matrix and make it time oriented. So let's take the essential Stockpile and vary it by time.\n",
    "\n",
    "What is easiest to do with the time series feature of is to flatten all the matrices. And then it seems you just need to add a timestamp.\n",
    "\n",
    "The other apporach is to use the so called multiindex which let's you do multi dimensionals in a 2-D way.\n",
    "\n",
    "The use of iterables makes it easy to mix the labels in [multiindexing](https://pandas.pydata.org/pandas-docs/stable/user_guide/advanced.html) but this is really a display thing. You cannot easily take a multidimensional cube and do a multiply. so the plan is to convert the multi-index to a numpy tensor do the math and then put it back.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DThfVZtG4Z7S",
    "colab_type": "code",
    "outputId": "3e57c070-a85a-44a2-d657-815118decdb4",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    }
   },
   "source": [
    "# https://stackoverflow.com/questions/25440008/python-pandas-flatten-a-dataframe-to-a-list\n",
    "Stockpile_by_essential_df.values.flatten()\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jggktC4lTNrs",
    "colab_type": "text"
   },
   "source": [
    "# Attachment: Test Code\n",
    "\n",
    "Used to test various features of the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H1uq0RMITXy3",
    "colab_type": "text"
   },
   "source": [
    "## Test of cloning an external Repo\n",
    "\n",
    "NOte that this does a complete clone in the virtual machine, make sure you have enough space. Also you need to reclone when you close a Notebook instance, so this can be slow with lots of data.\n",
    "\n",
    "However, it does allow you checkout particular branches and have a realiable dataset."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kD9qp04tU9-L",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Clone the entire repo.\n",
    "!git clone -l -s git://github.com/jakevdp/PythonDataScienceHandbook.git cloned-repo\n",
    "%cd cloned-repo\n",
    "!ls"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MDnksm1MTmV7",
    "colab_type": "text"
   },
   "source": [
    "## Test of copying a single file from a repo\n",
    "\n",
    "This one way to get small datasets, you just point to the raw file and use `!curl` to bring it into the machine."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Q7eqOYtkU99S",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Fetch a single <1MB file using the raw GitHub URL.\n",
    "!curl --remote-name \\\n",
    "     -H 'Accept: application/vnd.github.v3.raw' \\\n",
    "     --location https://api.github.com/repos/jakevdp/PythonDataScienceHandbook/contents/notebooks/data/california_cities.csv"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "38KeztwBajb8",
    "colab_type": "text"
   },
   "source": [
    "## Test of connecting to Google Drive\n",
    "\n",
    "This we can use if we don't need a repo, but are just loading a static file. We normally want everything from a repo or reliable storage, but this is good for quick analysis. In most cases, you should just check this into a repo and then use the github raw extract instead so you get version control.\n",
    "\n",
    "Note that this does require an authentication everytime you start the Notebook, so the raw extract works better particularly if there it is a public repo."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "pKyKRJeoHtJ_",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/gdrive')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "S2ldUhSATDRY",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "with open('/gdrive/My Drive/foo.txt', 'w') as f:\n",
    "  f.write('Hello Google Drive!')\n",
    "!cat '/gdrive/My Drive/foo.txt'"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gXxIvzREazRq",
    "colab_type": "text"
   },
   "source": [
    "## Connecting two cells together for summaries with Cross-output Communications\n",
    "\n",
    "This is the best method for connecting the longer analysis to a cell that just has the executive summary data. _This does not appear to be working. Need to debug_"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ndtWkGTjlL70",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "%%javascript\n",
    "const listenerChannel = new BroadcastChannel('channel');\n",
    "listenerChannel.onmessage = (msg) => {\n",
    "  const div = document.createElement('div');\n",
    "  div.textContent = msg.data;\n",
    "  document.body.appendChild(div);\n",
    "};"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kmwMtzEslL7J",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "%%javascript\n",
    "const senderChannel = new BroadcastChannel('channel');\n",
    "senderChannel.postMessage('Hello world!');"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7AulXwNtb5yd",
    "colab_type": "text"
   },
   "source": [
    "## Creating forms for entry\n",
    "\n",
    "This is going to be used to parameterize models. This sets global variables that can be used in cells farther down."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lXWX8aG7lWvD",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "#@title Example form fields\n",
    "#@markdown Forms support many types of fields.\n",
    "\n",
    "no_type_checking = ''  #@param\n",
    "string_type = 'example'  #@param {type: \"string\"}\n",
    "slider_value = 142  #@param {type: \"slider\", min: 100, max: 200}\n",
    "number = 102  #@param {type: \"number\"}\n",
    "date = '2010-11-05'  #@param {type: \"date\"}\n",
    "pick_me = \"monday\"  #@param ['monday', 'tuesday', 'wednesday', 'thursday']\n",
    "select_or_input = \"apples\" #@param [\"apples\", \"bananas\", \"oranges\"] {allow-input: true}\n",
    "#@markdown ---\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GDR12BB4pPd1",
    "colab_type": "text"
   },
   "source": [
    "## Display Pandas data dataframes use Vega datasets as an example\n",
    "\n",
    "This uses the extension `google.colab.data_table` and there is a default data set called `vega_datasets` where you can extract data. It is not clear where the data is or how to figure out how ot use it. Google-fu does not help although the [source code](https://github.com/googlecolab/colabtools/blob/master/google/colab/data_table.py) tells us that `vega_dataset` has airport data in it.\n",
    "\n",
    "But the hing is in the name Vega which is a visualization package and [Vega Datasets access from Python](https://github.com/jakevdp/vega_datasets) are a standard set of data for visualization testing. The core datasets are kept in [github.io](https://vega.github.io/vega-datasets/)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wW43_ntJpdVh",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "%load_ext google.colab.data_table"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "knd4KPzcpdUf",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "from vega_datasets import data\n",
    "data.cars()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KwkagE3vpdTY",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "%unload_ext google.colab.data_table"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OKvLbYFyy8J-",
    "colab_type": "text"
   },
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kgrbax2npdRf",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "data.stocks()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8evpmsNgsOaF",
    "colab_type": "text"
   },
   "source": [
    "## Github Rendering of Jupyter\n",
    "\n",
    "This is pretty cool, but [Github](https://help.github.com/en/github/managing-files-in-a-repository/working-with-jupyter-notebook-files-on-github) actually renders the Jupyter notebooks as statis HTML when you browse it. That means just clicking on a `.ipynb` will give you something reasonable. It is not interactive nor is anything running behind it, but it does mean that documents produced by use are easily readable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-RV8Y3kpyklo",
    "colab_type": "text"
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "CmBj6sJOykQe",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    ""
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}
